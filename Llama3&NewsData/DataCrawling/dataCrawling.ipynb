{"cells":[{"cell_type":"code","execution_count":null,"id":"beaf3b55-2974-4a5c-bbab-bbc353bc3031","metadata":{"id":"beaf3b55-2974-4a5c-bbab-bbc353bc3031"},"outputs":[],"source":["!pip install requests beautifulsoup4"]},{"cell_type":"code","execution_count":null,"id":"9e39a973-afca-47de-b840-a4445ea6ce78","metadata":{"id":"9e39a973-afca-47de-b840-a4445ea6ce78"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import urllib.parse\n","\n","# â”€â”€ ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì •\n","keyword = \"íŒŒì´ì¬\"\n","search_url = f\"https://www.jobkorea.co.kr/Search/?stext={urllib.parse.quote(keyword)}\"\n","\n","# â”€â”€ ìš”ì²­ í—¤ë” ì„¤ì • (í¬ë¡¤ë§ ì°¨ë‹¨ ìš°íšŒìš©)\n","HEADERS = {\n","    \"User-Agent\": (\n","        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n","        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n","        \"Chrome/114.0.0.0 Safari/537.36\"\n","    )\n","}\n","\n","# â”€â”€ HTML ê°€ì ¸ì˜¤ê¸°\n","resp = requests.get(search_url, headers=HEADERS)\n","soup = BeautifulSoup(resp.text, \"html.parser\")\n","\n","# â”€â”€ ê³µê³  ì œëª© ì¶”ì¶œ (êµ¬ì¡° ê¸°ë°˜ ì ‘ê·¼)\n","titles = []\n","\n","# a íƒœê·¸ ì¤‘ hrefê°€ \"/Recruit/GI_Read/\"ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒë§Œ\n","for a in soup.find_all(\"a\", href=True):\n","    if a[\"href\"].startswith(\"/Recruit/GI_Read/\"):\n","        title = a.get_text(strip=True)\n","        if title and len(title) > 5:\n","            titles.append(title)\n","        if len(titles) >= 5:\n","            break\n","\n","# â”€â”€ ì¶œë ¥\n","print(\"ğŸ” ì¡ì½”ë¦¬ì•„ ì±„ìš© ê³µê³  (ìƒìœ„ 5ê°œ):\")\n","for i, t in enumerate(titles, 1):\n","    print(f\"{i}. {t}\")\n"]},{"cell_type":"code","execution_count":null,"id":"33650df5-5d66-4450-be65-80b700919dad","metadata":{"id":"33650df5-5d66-4450-be65-80b700919dad"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import urllib.parse\n","\n","# â”€â”€ ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì •\n","keyword = \"íŒŒì´ì¬\"\n","search_url = f\"https://www.jobkorea.co.kr/Search/?stext={urllib.parse.quote(keyword)}\"\n","\n","# â”€â”€ ìš”ì²­ í—¤ë” ì„¤ì • (í¬ë¡¤ë§ ì°¨ë‹¨ ìš°íšŒìš©)\n","HEADERS = {\n","    \"User-Agent\": (\n","        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n","        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n","        \"Chrome/114.0.0.0 Safari/537.36\"\n","    )\n","}\n","\n","# â”€â”€ 1) HTML ê°€ì ¸ì˜¤ê¸° & ìƒíƒœ í™•ì¸\n","resp = requests.get(search_url, headers=HEADERS, timeout=10)\n","print(\"ğŸ” HTTP ìƒíƒœ ì½”ë“œ:\", resp.status_code)\n","if resp.status_code != 200:\n","    print(\"! ERROR: í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n","    exit(1)\n","\n","html = resp.text\n","print(\"ğŸ” HTML ì²˜ìŒ 500ì:\\n\", html[:500].replace(\"\\n\", \" \"), \"...\\n\")\n","\n","# â”€â”€ 2) BeautifulSoup íŒŒì‹± í›„ ì „ì²´ <a> íƒœê·¸ ê°œìˆ˜ í™•ì¸\n","soup = BeautifulSoup(html, \"html.parser\")\n","all_a = soup.find_all(\"a\", href=True)\n","print(f\"ğŸ” ì „ì²´ <a> íƒœê·¸ ê°œìˆ˜: {len(all_a)}\")\n","print(\"ğŸ” ì²˜ìŒ 20ê°œ href/text ìƒ˜í”Œ:\")\n","for i, a in enumerate(all_a[:20], 1):\n","    print(f\"  {i:2d}. href={a['href']!r}, text={a.get_text(strip=True)!r}\")\n","print()\n","\n","# â”€â”€ 3) í•„í„°ë§ëœ ê³µê³  ì œëª© ì¶”ì¶œ\n","titles = []\n","for a in all_a:\n","    href = a[\"href\"]\n","    if href.startswith(\"/Recruit/GI_Read/\"):\n","        print(\"âœ”ï¸ ë§¤ì¹­ëœ href:\", href)  # í•„í„° ì¡°ê±´ì— ê±¸ë¦´ ë•Œë§ˆë‹¤ ì¶œë ¥\n","        title = a.get_text(strip=True)\n","        print(\"   text:\", title)\n","        if title and len(title) > 5:\n","            titles.append(title)\n","        if len(titles) >= 5:\n","            break\n","\n","# â”€â”€ 4) ê²°ê³¼ ì¶œë ¥\n","if titles:\n","    print(\"\\nğŸ” ì¡ì½”ë¦¬ì•„ ì±„ìš© ê³µê³  (ìƒìœ„ 5ê°œ):\")\n","    for i, t in enumerate(titles, 1):\n","        print(f\"{i}. {t}\")\n","else:\n","    print(\"\\n! WARNING: í•„í„°ë§ëœ ê³µê³ ë¥¼ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n"]},{"cell_type":"code","execution_count":null,"id":"24d86772-9dbd-44fc-b2b3-7f263cb73360","metadata":{"id":"24d86772-9dbd-44fc-b2b3-7f263cb73360"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import urllib.parse\n","\n","# â”€â”€ ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì •\n","keyword = \"íŒŒì´ì¬\"\n","base_url = \"https://www.jobkorea.co.kr\"\n","search_url = f\"{base_url}/Search/?stext={urllib.parse.quote(keyword)}\"\n","\n","# â”€â”€ ìš”ì²­ í—¤ë” ì„¤ì • (í¬ë¡¤ë§ ì°¨ë‹¨ ìš°íšŒìš©)\n","HEADERS = {\n","    \"User-Agent\": (\n","        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n","        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n","        \"Chrome/114.0.0.0 Safari/537.36\"\n","    )\n","}\n","\n","# â”€â”€ 1) HTML ê°€ì ¸ì˜¤ê¸°\n","resp = requests.get(search_url, headers=HEADERS, timeout=10)\n","print(\"ğŸ” HTTP ìƒíƒœ ì½”ë“œ:\", resp.status_code)\n","html = resp.text\n","\n","# â”€â”€ 2) BeautifulSoup íŒŒì‹±\n","soup = BeautifulSoup(html, \"html.parser\")\n","all_a = soup.find_all(\"a\", href=True)\n","print(f\"ğŸ” ì „ì²´ <a> íƒœê·¸ ê°œìˆ˜: {len(all_a)}\")\n","\n","# â”€â”€ 3) ê³µê³  ë§í¬/ì œëª© ìˆ˜ì§‘\n","titles = []\n","seen_urls = set()\n","\n","for a in all_a:\n","    href = a[\"href\"]\n","    # \"/Recruit/GI_Read/\"ê°€ í¬í•¨ëœ ë§í¬ë¼ë©´ ì ˆëŒ€/ìƒëŒ€ ëª¨ë‘ ì¡ê¸°\n","    if \"/Recruit/GI_Read/\" in href:\n","        # ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜\n","        full_url = urllib.parse.urljoin(base_url, href)\n","        if full_url in seen_urls:\n","            continue\n","        seen_urls.add(full_url)\n","\n","        title = a.get_text(strip=True)\n","        print(f\"âœ”ï¸ ë°œê²¬: URL={full_url}, text={title!r}\")\n","\n","        # ì›í•˜ëŠ” ìµœì†Œ ê¸¸ì´ë§Œ ë‚¨ê¸°ê³ \n","        if title and len(title) > 10:\n","            titles.append((title, full_url))\n","\n","        if len(titles) >= 5:\n","            break\n","\n","# â”€â”€ 4) ê²°ê³¼ ì¶œë ¥\n","if titles:\n","    print(\"\\nğŸ” ì¡ì½”ë¦¬ì•„ ì±„ìš© ê³µê³  (ìƒìœ„ 5ê°œ):\")\n","    for i, (t, u) in enumerate(titles, 1):\n","        print(f\"{i}. {t}\\n   {u}\\n\")\n","else:\n","    print(\"\\n! WARNING: í•„í„°ë§ëœ ê³µê³ ë¥¼ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n"]},{"cell_type":"code","execution_count":null,"id":"823646a6-932d-4ba3-8f82-2d5833403c0f","metadata":{"id":"823646a6-932d-4ba3-8f82-2d5833403c0f"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import urllib.parse\n","\n","# â”€â”€ ì‚¬ìš©ì ì…ë ¥\n","keyword = input('í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ')\n","page_num = int(input('í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”: '))  # ìˆ«ìë¡œ ë°”ë¡œ ë³€í™˜\n","base_url = \"https://www.jobkorea.co.kr\"\n","\n","# â”€â”€ ìš”ì²­ í—¤ë”\n","HEADERS = {\n","    \"User-Agent\": (\n","        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n","        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n","        \"Chrome/114.0.0.0 Safari/537.36\"\n","    )\n","}\n","\n","# â”€â”€ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸/ì§‘í•©ì€ ë£¨í”„ ë°–ì—ì„œ ì„ ì–¸!\n","titles = []\n","seen = set()\n","\n","for n in range(1, page_num + 1):           # 1ë¶€í„° page_numê¹Œì§€\n","    search_url = (\n","        f\"{base_url}/Search/?\"\n","        f\"stext={urllib.parse.quote(keyword)}&\"\n","        f\"tabType=recruit&Page_No={n}\"\n","    )\n","    try:\n","        resp = requests.get(search_url, headers=HEADERS, timeout=10)\n","        # HTTP ìš”ì²­ì„ ë³´ë‚¸ ë’¤ ì‘ë‹µ ì½”ë“œê°€ 200ë²ˆëŒ€(ì„±ê³µ)ì´ ì•„ë‹Œ ê²½ìš°ì— ì¦‰ì‹œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œì¤Œ.\n","        # ë§Œì•½ 404ì¸ ì—ëŸ¬ê°€ í„°ì¡Œì„ê²½ìš° ë°”ë¡œ ì•„ë˜ì— ìˆëŠ” exceptë¸”ë¡ì—ì„œ ì¡ì•„ì„œ ì²˜ë¦¬í•¨.\n","        resp.raise_for_status()\n","        soup = BeautifulSoup(resp.text, \"html.parser\")\n","    except Exception as e:\n","        print(f\"[í˜ì´ì§€ {n}] ìš”ì²­ ì‹¤íŒ¨:\", e)\n","        continue\n","\n","    # â”€â”€ a íƒœê·¸ ì¤‘ ì±„ìš©ê³µê³  ë§í¬ë§Œ ê³¨ë¼ì„œ\n","    for a in soup.select(\"a[href*='/Recruit/GI_Read/']\"):\n","        href = a[\"href\"]\n","        full_url = urllib.parse.urljoin(base_url, href)\n","\n","        if full_url in seen:\n","            continue\n","        seen.add(full_url)\n","\n","        # â”€â”€ <a> ë°”ë¡œ ì•„ë˜ <span> ì— ì œëª©ì´ ê°ì‹¸ì—¬ ìˆë‹¤ë©´\n","        span = a.find(\"span\")\n","        if not span:\n","            continue  # spanì´ ì—†ìœ¼ë©´ ë‹¤ìŒ ë§í¬ë¡œ\n","\n","        title = span.get_text(strip=True)\n","        if len(title) < 5:\n","            continue\n","\n","        titles.append((title, full_url))\n","\n","# â”€â”€ ì¶œë ¥\n","print(f\"ğŸ” ì¡ì½”ë¦¬ì•„ ì±„ìš© ê³µê³  (ì´ {len(titles)}ê±´):\")\n","for i, (t, u) in enumerate(titles, 1):\n","    print(f\"{i}. {t}\\n   {u}\\n\")\n"]},{"cell_type":"code","execution_count":null,"id":"13b277c9-ba00-41c8-a6f8-d3c6eef1b3f9","metadata":{"id":"13b277c9-ba00-41c8-a6f8-d3c6eef1b3f9"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import re\n","\n","url = (\n","    \"https://www.jobkorea.co.kr/Recruit/GI_Read/\"\n","    \"47370797?Oem_Code=C1&logpath=1&\"\n","    \"stext=%ED%8C%8C%EC%9D%B4%EC%8D%AC&\"\n","    \"listno=2&sc=631\"\n",")\n","headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","resp = requests.get(url, headers=headers)\n","soup = BeautifulSoup(resp.text, \"html.parser\")\n","\n","# 1) íšŒì‚¬ëª… ì¶”ì¶œ: í´ë˜ìŠ¤ëª…ì— 'coName'ì´ í¬í•¨ëœ span\n","co_tag = soup.find(\"span\", class_=re.compile(r\"coName\"))\n","company_name = co_tag.get_text(strip=True) if co_tag else \"ì¶”ì¶œ ì‹¤íŒ¨\"\n","print(\"íšŒì‚¬ëª…:\", company_name)\n","\n","# 2) ê³µê³  ì œëª© ì¶”ì¶œ:\n","#    h3 íƒœê·¸ ì¤‘ í´ë˜ìŠ¤ì— 'hd_'ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒ ì°¾ê¸°\n","h3 = soup.find(\"h3\", class_=re.compile(r\"hd_\"))\n","job_title = None\n","if h3:\n","    # h3íƒœê·¸ ë‚´ë¶€ì— ìˆëŠ” ëª¨ë“  spaníƒœê·¸ ì œê±°\n","    for span in h3.find_all(\"span\"):\n","        span.extract()\n","    # h3íƒœê·¸ ë‚´ë¶€ì— ìˆëŠ” píƒœê·¸ì¤‘ classê°€ 'txt'ì¸ì• ë“¤ë§Œ ì œê±°\n","    for p_txt in h3.find_all(\"p\", class_=re.compile(r\"txt\")):\n","        p_txt.extract()\n","    # ë‚¨ì€ í…ìŠ¤íŠ¸ê°€ ì œëª©\n","    job_title = h3.get_text(strip=True)\n","print(\"ê³µê³  ì œëª©:\", job_title or \"ì¶”ì¶œ ì‹¤íŒ¨\")\n","\n","# 4) ìƒì„¸ ì§€ì›ìê²©(ê²½ë ¥, í•™ë ¥, ìŠ¤í‚¬) ì¶”ì¶œ\n","qual_details = {}\n","# tbRow clear í´ë˜ìŠ¤ ê°€ì§„ div ì „ë¶€ ìˆœíšŒ\n","for div in soup.find_all(\"div\", class_=re.compile(r\"tbRow.*clear\")):\n","    h4 = div.find(\"h4\")\n","    # h4 í…ìŠ¤íŠ¸ì— 'ì§€ì›ìê²©'ì´ í¬í•¨ëœ ë¸”ë¡ë§Œ ì„ íƒ\n","    if h4 and \"ì§€ì›ìê²©\" in h4.get_text():\n","        dl = div.find(\"dl\", class_=\"tbList\")\n","        if dl:\n","            # dt/dd ìŒìœ¼ë¡œ ìˆœíšŒí•˜ë©° ë”•ì…”ë„ˆë¦¬ì— ì €ì¥\n","            for dt, dd in zip(dl.find_all(\"dt\"), dl.find_all(\"dd\")):\n","                key = dt.get_text(strip=True)\n","                val = dd.get_text(strip=True)\n","                qual_details[key] = val\n","        break\n","\n","qual = extract_section(\"ì§€ì›ìê²©\")\n","work = extract_section(\"ê·¼ë¬´ì¡°ê±´\")\n","print(\"ì§€ì›ìê²© ìƒì„¸:\")\n","for k, v in qual_details.items():\n","    print(f\" - {k}: {v}\")\n"]},{"cell_type":"code","execution_count":null,"id":"0fc2f2a9-879b-4e18-9bed-d50f705d2b13","metadata":{"id":"0fc2f2a9-879b-4e18-9bed-d50f705d2b13"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import re\n","\n","url = (\n","    \"https://www.jobkorea.co.kr/Recruit/GI_Read/\"\n","    \"47370797?Oem_Code=C1&logpath=1&\"\n","    \"stext=%ED%8C%8C%EC%9D%B4%EC%8D%AC&\"\n","    \"listno=2&sc=631\"\n",")\n","headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","resp = requests.get(url, headers=headers)\n","print(\"ğŸ” HTTP ìƒíƒœ ì½”ë“œ:\", resp.status_code)\n","\n","html = resp.text\n","print(\"\\nâ”€â”€ HTML ì²˜ìŒ 500ì â”€â”€\")\n","print(html[:500].replace(\"\\n\", \" \"), \"\\nâ€¦\\n\")\n","\n","soup = BeautifulSoup(html, \"html.parser\")\n","\n","# 1) ëª¨ë“  span íƒœê·¸ì™€ í´ë˜ìŠ¤ ì°ì–´ë³´ê¸°\n","print(\"â”€â”€ ALL <span> TAGS & CLASSES â”€â”€\")\n","for span in soup.find_all(\"span\"):\n","    print(\"span:\", repr(span.get_text(strip=True)), \"| class:\", span.get(\"class\"))\n","print()\n","\n","# 2) 'coName' ë°©ì‹ìœ¼ë¡œ ì°¾ëŠ” span íƒœê·¸ ì°ì–´ë³´ê¸°\n","print(\"â”€â”€ FIND span[class*=coName] â”€â”€\")\n","for span in soup.find_all(\"span\", class_=re.compile(r\"coName\")):\n","    print(span)\n","print()\n","\n","# 3) ëª¨ë“  h3 íƒœê·¸ì™€ í´ë˜ìŠ¤ ì°ì–´ë³´ê¸°\n","print(\"â”€â”€ ALL <h3> TAGS & CLASSES â”€â”€\")\n","for h3 in soup.find_all(\"h3\"):\n","    print(\"h3 class:\", h3.get(\"class\"), \"| html:\", h3.prettify())\n","print()\n","\n","# 4) 'hd_' íŒ¨í„´ìœ¼ë¡œ ì°¾ëŠ” h3 íƒœê·¸ ì°ì–´ë³´ê¸°\n","print(\"â”€â”€ FIND h3[class^=hd_] â”€â”€\")\n","for h3 in soup.find_all(\"h3\", class_=re.compile(r\"hd_\")):\n","    print(h3.prettify())\n","print()\n","\n","# 5) tbRow clear ë¸”ë¡ ì°ì–´ë³´ê¸°\n","print(\"â”€â”€ FIND div[class*=tbRow] â”€â”€\")\n","for div in soup.find_all(\"div\", class_=re.compile(r\"tbRow\")):\n","    h4 = div.find(\"h4\")\n","    print(\"BLOCK h4 text:\", h4.get_text(strip=True) if h4 else \"<no h4>\")\n","    print(div.prettify(), \"\\n---\\n\")\n"]},{"cell_type":"code","execution_count":null,"id":"179ce7b0-8ff8-437f-a190-59ea5d30f579","metadata":{"id":"179ce7b0-8ff8-437f-a190-59ea5d30f579"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import urllib.parse\n","import re\n","\n","# â”€â”€ ì„¤ì •\n","HEADERS = {\n","    \"User-Agent\": (\n","        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n","        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n","        \"Chrome/114.0.0.0 Safari/537.36\"\n","    )\n","}\n","BASE_URL = \"https://www.jobkorea.co.kr\"\n","\n","def get_job_links(keyword, page_num):\n","    links = []\n","    seen = set()\n","    for n in range(1, page_num + 1):\n","        search_url = (\n","            f\"{BASE_URL}/Search/?\"\n","            f\"stext={urllib.parse.quote(keyword)}&\"\n","            f\"tabType=recruit&Page_No={n}\"\n","        )\n","        try:\n","            resp = requests.get(search_url, headers=HEADERS, timeout=10)\n","            resp.raise_for_status()\n","        except Exception as e:\n","            print(f\"[í˜ì´ì§€ {n}] ìš”ì²­ ì‹¤íŒ¨:\", e)\n","            continue\n","\n","        soup = BeautifulSoup(resp.text, \"html.parser\")\n","        for a in soup.select(\"a[href*='/Recruit/GI_Read/']\"):\n","            href = a[\"href\"]\n","            full_url = urllib.parse.urljoin(BASE_URL, href)\n","            if full_url in seen:\n","                continue\n","            seen.add(full_url)\n","\n","            span = a.find(\"span\")\n","            if not span:\n","                continue\n","            title = span.get_text(strip=True)\n","            if len(title) < 5:\n","                continue\n","\n","            links.append((title, full_url))\n","    return links\n","\n","def parse_job_detail(url):\n","    try:\n","        resp = requests.get(url, headers=HEADERS, timeout=10)\n","        resp.raise_for_status()\n","    except Exception as e:\n","        print(f\"[ìƒì„¸] {url} ìš”ì²­ ì‹¤íŒ¨:\", e)\n","        return None\n","\n","    soup = BeautifulSoup(resp.text, \"html.parser\")\n","\n","    co_tag = soup.find(\"span\", class_=re.compile(r\"coName\"))\n","    company_name = co_tag.get_text(strip=True) if co_tag else \"\"\n","\n","    h3 = soup.find(\"h3\", class_=re.compile(r\"hd_\"))\n","    job_title = \"\"\n","    if h3:\n","        for span in h3.find_all(\"span\"):\n","            span.extract()\n","        for p_txt in h3.find_all(\"p\", class_=re.compile(r\"txt\")):\n","            p_txt.extract()\n","        job_title = h3.get_text(strip=True)\n","\n","    def extract_section(title):\n","        tit = soup.find(\"div\", string=re.compile(title))\n","        if tit:\n","            content = tit.find_next_sibling(\"div\")\n","            return content.get_text(strip=True) if content else \"\"\n","        return \"\"\n","    simple_qual = extract_section(\"ì§€ì›ìê²©\")\n","    work = extract_section(\"ê·¼ë¬´ì¡°ê±´\")\n","\n","    qual_details = {}\n","    for div in soup.find_all(\"div\", class_=re.compile(r\"tbRow.*clear\")):\n","        h4 = div.find(\"h4\")\n","        if h4 and \"ì§€ì›ìê²©\" in h4.get_text():\n","            dl = div.find(\"dl\", class_=\"tbList\")\n","            if dl:\n","                for dt, dd in zip(dl.find_all(\"dt\"), dl.find_all(\"dd\")):\n","                    qual_details[dt.get_text(strip=True)] = dd.get_text(strip=True)\n","            break\n","\n","    return {\n","        \"company_name\": company_name,\n","        \"job_title\": job_title,\n","        \"simple_qual\": simple_qual,\n","        \"work\": work,\n","        \"qual_details\": qual_details\n","    }\n","\n","def main():\n","    keyword = input(\"í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: \")\n","    page_num = int(input(\"í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”: \"))\n","    links = get_job_links(keyword, page_num)\n","\n","    with open(\"jobKorea.txt\", \"w\", encoding=\"utf-8\") as f:\n","        for idx, (_, url) in enumerate(links, 1):\n","            f.write(f\"=== ê³µê³  {idx} ===\\n\")\n","\n","            data = parse_job_detail(url)\n","            if not data:\n","                f.write(\"ìƒì„¸ ì •ë³´ íŒŒì‹± ì‹¤íŒ¨\\n\\n\")\n","                continue\n","\n","            f.write(f\"íšŒì‚¬ëª…: {data['company_name']}\\n\")\n","            f.write(f\"ê³µê³  ì œëª©: {data['job_title']}\\n\")\n","            f.write(f\"ì§€ì›ìê²© (ìš”ì•½): {data['simple_qual']}\\n\")\n","            f.write(f\"ê·¼ë¬´ì¡°ê±´: {data['work']}\\n\")\n","            f.write(\"ì§€ì›ìê²© ìƒì„¸:\\n\")\n","            for k, v in data[\"qual_details\"].items():\n","                f.write(f\"  - {k}: {v}\\n\")\n","            f.write(\"\\n\")\n","\n","    print(f\"ì™„ë£Œ: jobKorea.txt íŒŒì¼ì— {len(links)}ê±´ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"id":"ed04ecb6-be02-416b-8b54-e605e3e54c9a","metadata":{"id":"ed04ecb6-be02-416b-8b54-e605e3e54c9a"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}
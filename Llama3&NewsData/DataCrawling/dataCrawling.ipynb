{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beaf3b55-2974-4a5c-bbab-bbc353bc3031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\anaconda\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\anaconda\\lib\\site-packages (from beautifulsoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e39a973-afca-47de-b840-a4445ea6ce78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ì¡ì½”ë¦¬ì•„ ì±„ìš© ê³µê³  (ìƒìœ„ 5ê°œ):\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "\n",
    "# â”€â”€ ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì •\n",
    "keyword = \"íŒŒì´ì¬\"\n",
    "search_url = f\"https://www.jobkorea.co.kr/Search/?stext={urllib.parse.quote(keyword)}\"\n",
    "\n",
    "# â”€â”€ ìš”ì²­ í—¤ë” ì„¤ì • (í¬ë¡¤ë§ ì°¨ë‹¨ ìš°íšŒìš©)\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# â”€â”€ HTML ê°€ì ¸ì˜¤ê¸°\n",
    "resp = requests.get(search_url, headers=HEADERS)\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "# â”€â”€ ê³µê³  ì œëª© ì¶”ì¶œ (êµ¬ì¡° ê¸°ë°˜ ì ‘ê·¼)\n",
    "titles = []\n",
    "\n",
    "# a íƒœê·¸ ì¤‘ hrefê°€ \"/Recruit/GI_Read/\"ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒë§Œ\n",
    "for a in soup.find_all(\"a\", href=True):\n",
    "    if a[\"href\"].startswith(\"/Recruit/GI_Read/\"):\n",
    "        title = a.get_text(strip=True)\n",
    "        if title and len(title) > 5:\n",
    "            titles.append(title)\n",
    "        if len(titles) >= 5:\n",
    "            break\n",
    "\n",
    "# â”€â”€ ì¶œë ¥\n",
    "print(\"ğŸ” ì¡ì½”ë¦¬ì•„ ì±„ìš© ê³µê³  (ìƒìœ„ 5ê°œ):\")\n",
    "for i, t in enumerate(titles, 1):\n",
    "    print(f\"{i}. {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33650df5-5d66-4450-be65-80b700919dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” HTTP ìƒíƒœ ì½”ë“œ: 200\n",
      "ğŸ” HTML ì²˜ìŒ 500ì:\n",
      " <!DOCTYPE html><html lang=\"ko\" data-sentry-component=\"RootLayout\" data-sentry-source-file=\"layout.tsx\"><head translate=\"no\"><meta charSet=\"utf-8\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no\"/><link rel=\"stylesheet\" href=\"https://frontend-app-cdn.jobkorea.co.kr/jobs/_next/static/css/5951755f53fa0018.css\" data-precedence=\"next\"/><link rel=\"stylesheet\" href=\"https://frontend-app-cdn.jobkorea.co.kr/jobs/_next/static/css/09f47635fc16a486.css\" ...\n",
      "\n",
      "ğŸ” ì „ì²´ <a> íƒœê·¸ ê°œìˆ˜: 57\n",
      "ğŸ” ì²˜ìŒ 20ê°œ href/text ìƒ˜í”Œ:\n",
      "   1. href='#main', text='Skip to main content'\n",
      "   2. href='https://www.jobkorea.co.kr', text=''\n",
      "   3. href='https://www.jobkorea.co.kr/Login/Login_Tot.asp?re_url=%2FSearch%3Fstext%3D%25ED%258C%258C%25EC%259D%25B4%25EC%258D%25AC', text='ë¡œê·¸ì¸'\n",
      "   4. href='https://www.jobkorea.co.kr/Join/M_Regist', text='íšŒì›ê°€ì…'\n",
      "   5. href='https://www.jobkorea.co.kr/Recruit/GI_Read/47279172?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=1&sc=632', text='í•«ì…€ëŸ¬'\n",
      "   6. href='https://www.jobkorea.co.kr/Recruit/GI_Read/47279172?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=1&sc=632', text='Python ë°±ì—”ë“œ ê°œë°œì ì±„ìš©'\n",
      "   7. href='https://www.jobkorea.co.kr/Recruit/GI_Read/47370797?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=2&sc=631', text='ãˆœì œì´íˆ¬ì´'\n",
      "   8. href='https://www.jobkorea.co.kr/Recruit/GI_Read/47370797?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=2&sc=631', text='[PYTHON SI ì—¬ì˜ë„]  00ì€í–‰ AI ì—ì´ì „íŠ¸ ê°œë°œ  (6)'\n",
      "   9. href='https://www.jobkorea.co.kr/Recruit/GI_Read/47334462?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=3&sc=631', text='ãˆœì”¨ì–´ìŠ¤í…Œí¬ë†€ë¡œì§€'\n",
      "  10. href='https://www.jobkorea.co.kr/Recruit/GI_Read/47334462?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=3&sc=631', text='[ê²½ë ¥] Python ê°œë°œì'\n",
      "  11. href='https://www.jobkorea.co.kr/Recruit/GI_Read/47026474?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=4&sc=631', text='ãˆœë¹„ì „ì½”ì–´'\n",
      "  12. href='https://www.jobkorea.co.kr/Recruit/GI_Read/47026474?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=4&sc=631', text='íŒŒì´ì¬(Python), ì•±(ios/ì•ˆë“œë¡œì´ë“œ) ê°œë°œì ëª¨ì§‘'\n",
      "  13. href='https://www.jobkorea.co.kr/Recruit/GI_Read/47337684?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=5&sc=631', text='ì—°ì„¸ITë¯¸ë˜êµìœ¡ì›'\n",
      "  14. href='https://www.jobkorea.co.kr/Recruit/GI_Read/47337684?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=5&sc=631', text='ìë°” /íŒŒì´ì¬(ë‹¨ê¸°) ì‹œê°„ê°•ì‚¬ ëª¨ì§‘'\n",
      "  15. href='https://www.jobkorea.co.kr/Recruit/GI_Read/47308844?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=6&sc=631', text='ãˆœë§í¬ì œë‹ˆì‹œìŠ¤'\n",
      "  16. href='https://www.jobkorea.co.kr/Recruit/GI_Read/47308844?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=6&sc=631', text='ì°¨ëŸ‰ìš© SW Test ìë™í™” ê²€ì¦(Python)'\n",
      "  17. href='https://www.jobkorea.co.kr/Recruit/GI_Read/47374290?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=7&sc=631', text='ãˆœì²«ëˆˆì†Œí”„íŠ¸'\n",
      "  18. href='https://www.jobkorea.co.kr/Recruit/GI_Read/47374290?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=7&sc=631', text='íŒŒì´ì¬/Pythonê²½í—˜ì'\n",
      "  19. href='https://www.jobkorea.co.kr/Recruit/GI_Read/47364099?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=8&sc=631', text='íŒŒì›Œì œë„¥ìŠ¤ãˆœ'\n",
      "  20. href='https://www.jobkorea.co.kr/Recruit/GI_Read/47364099?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=8&sc=631', text='ìë™ ì œì–´ í”„ë¡œê·¸ë¨(python) ê°œë°œì ì±„ìš©'\n",
      "\n",
      "\n",
      "! WARNING: í•„í„°ë§ëœ ê³µê³ ë¥¼ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "\n",
    "# â”€â”€ ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì •\n",
    "keyword = \"íŒŒì´ì¬\"\n",
    "search_url = f\"https://www.jobkorea.co.kr/Search/?stext={urllib.parse.quote(keyword)}\"\n",
    "\n",
    "# â”€â”€ ìš”ì²­ í—¤ë” ì„¤ì • (í¬ë¡¤ë§ ì°¨ë‹¨ ìš°íšŒìš©)\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# â”€â”€ 1) HTML ê°€ì ¸ì˜¤ê¸° & ìƒíƒœ í™•ì¸\n",
    "resp = requests.get(search_url, headers=HEADERS, timeout=10)\n",
    "print(\"ğŸ” HTTP ìƒíƒœ ì½”ë“œ:\", resp.status_code)\n",
    "if resp.status_code != 200:\n",
    "    print(\"! ERROR: í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    exit(1)\n",
    "\n",
    "html = resp.text\n",
    "print(\"ğŸ” HTML ì²˜ìŒ 500ì:\\n\", html[:500].replace(\"\\n\", \" \"), \"...\\n\")\n",
    "\n",
    "# â”€â”€ 2) BeautifulSoup íŒŒì‹± í›„ ì „ì²´ <a> íƒœê·¸ ê°œìˆ˜ í™•ì¸\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "all_a = soup.find_all(\"a\", href=True)\n",
    "print(f\"ğŸ” ì „ì²´ <a> íƒœê·¸ ê°œìˆ˜: {len(all_a)}\")\n",
    "print(\"ğŸ” ì²˜ìŒ 20ê°œ href/text ìƒ˜í”Œ:\")\n",
    "for i, a in enumerate(all_a[:20], 1):\n",
    "    print(f\"  {i:2d}. href={a['href']!r}, text={a.get_text(strip=True)!r}\")\n",
    "print()\n",
    "\n",
    "# â”€â”€ 3) í•„í„°ë§ëœ ê³µê³  ì œëª© ì¶”ì¶œ\n",
    "titles = []\n",
    "for a in all_a:\n",
    "    href = a[\"href\"]\n",
    "    if href.startswith(\"/Recruit/GI_Read/\"):\n",
    "        print(\"âœ”ï¸ ë§¤ì¹­ëœ href:\", href)  # í•„í„° ì¡°ê±´ì— ê±¸ë¦´ ë•Œë§ˆë‹¤ ì¶œë ¥\n",
    "        title = a.get_text(strip=True)\n",
    "        print(\"   text:\", title)\n",
    "        if title and len(title) > 5:\n",
    "            titles.append(title)\n",
    "        if len(titles) >= 5:\n",
    "            break\n",
    "\n",
    "# â”€â”€ 4) ê²°ê³¼ ì¶œë ¥\n",
    "if titles:\n",
    "    print(\"\\nğŸ” ì¡ì½”ë¦¬ì•„ ì±„ìš© ê³µê³  (ìƒìœ„ 5ê°œ):\")\n",
    "    for i, t in enumerate(titles, 1):\n",
    "        print(f\"{i}. {t}\")\n",
    "else:\n",
    "    print(\"\\n! WARNING: í•„í„°ë§ëœ ê³µê³ ë¥¼ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d86772-9dbd-44fc-b2b3-7f263cb73360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” HTTP ìƒíƒœ ì½”ë“œ: 200\n",
      "ğŸ” ì „ì²´ <a> íƒœê·¸ ê°œìˆ˜: 57\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47279172?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=1&sc=632, text='í•«ì…€ëŸ¬'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47370797?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=2&sc=631, text='ãˆœì œì´íˆ¬ì´'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47334462?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=3&sc=631, text='ãˆœì”¨ì–´ìŠ¤í…Œí¬ë†€ë¡œì§€'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47026474?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=4&sc=631, text='ãˆœë¹„ì „ì½”ì–´'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47337684?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=5&sc=631, text='ì—°ì„¸ITë¯¸ë˜êµìœ¡ì›'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47308844?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=6&sc=631, text='ãˆœë§í¬ì œë‹ˆì‹œìŠ¤'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47374290?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=7&sc=631, text='ãˆœì²«ëˆˆì†Œí”„íŠ¸'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47300396?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=8&sc=631, text='ãˆœì• ë“œí¬ë¦¼'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47364099?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=9&sc=631, text='íŒŒì›Œì œë„¥ìŠ¤ãˆœ'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47385516?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=10&sc=631, text='ãˆœë¹„ì—ì´ì†Œí”„íŠ¸'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47332690?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=11&sc=631, text='ãˆœí¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47155591?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=12&sc=631, text='ê·¸ë¦°ì—ì½”ìŠ¤ãˆœ'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47344352?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=13&sc=631, text='ë§ˆì´ë°ì´í„°'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47373865?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=14&sc=631, text='ë“œë¦¼ë‚˜ìš°ãˆœ'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47350203?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=15&sc=631, text='ãˆœê°€ì˜¨ë°ì´í„°'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47230250?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=16&sc=631, text='ãˆœìš°ë¦¬ì¸ì¬ê°œë°œì›'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47360710?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=17&sc=631, text='ãˆœë”¥ë…¸ì´ë“œ'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47373876?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=18&sc=631, text='ë“œë¦¼ë‚˜ìš°ãˆœ'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47222665?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=19&sc=631, text='íë¹„íŠ¸ì˜¨ãˆœ'\n",
      "âœ”ï¸ ë°œê²¬: URL=https://www.jobkorea.co.kr/Recruit/GI_Read/47383141?Oem_Code=C1&logpath=1&stext=íŒŒì´ì¬&listno=20&sc=631, text='ãˆœë¬´í•œì •ë³´ê¸°ìˆ '\n",
      "\n",
      "! WARNING: í•„í„°ë§ëœ ê³µê³ ë¥¼ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "\n",
    "# â”€â”€ ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì •\n",
    "keyword = \"íŒŒì´ì¬\"\n",
    "base_url = \"https://www.jobkorea.co.kr\"\n",
    "search_url = f\"{base_url}/Search/?stext={urllib.parse.quote(keyword)}\"\n",
    "\n",
    "# â”€â”€ ìš”ì²­ í—¤ë” ì„¤ì • (í¬ë¡¤ë§ ì°¨ë‹¨ ìš°íšŒìš©)\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# â”€â”€ 1) HTML ê°€ì ¸ì˜¤ê¸°\n",
    "resp = requests.get(search_url, headers=HEADERS, timeout=10)\n",
    "print(\"ğŸ” HTTP ìƒíƒœ ì½”ë“œ:\", resp.status_code)\n",
    "html = resp.text\n",
    "\n",
    "# â”€â”€ 2) BeautifulSoup íŒŒì‹±\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "all_a = soup.find_all(\"a\", href=True)\n",
    "print(f\"ğŸ” ì „ì²´ <a> íƒœê·¸ ê°œìˆ˜: {len(all_a)}\")\n",
    "\n",
    "# â”€â”€ 3) ê³µê³  ë§í¬/ì œëª© ìˆ˜ì§‘\n",
    "titles = []\n",
    "seen_urls = set()\n",
    "\n",
    "for a in all_a:\n",
    "    href = a[\"href\"]\n",
    "    # \"/Recruit/GI_Read/\"ê°€ í¬í•¨ëœ ë§í¬ë¼ë©´ ì ˆëŒ€/ìƒëŒ€ ëª¨ë‘ ì¡ê¸°\n",
    "    if \"/Recruit/GI_Read/\" in href:\n",
    "        # ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜\n",
    "        full_url = urllib.parse.urljoin(base_url, href)\n",
    "        if full_url in seen_urls:\n",
    "            continue\n",
    "        seen_urls.add(full_url)\n",
    "\n",
    "        title = a.get_text(strip=True)\n",
    "        print(f\"âœ”ï¸ ë°œê²¬: URL={full_url}, text={title!r}\")\n",
    "\n",
    "        # ì›í•˜ëŠ” ìµœì†Œ ê¸¸ì´ë§Œ ë‚¨ê¸°ê³ \n",
    "        if title and len(title) > 10:\n",
    "            titles.append((title, full_url))\n",
    "\n",
    "        if len(titles) >= 5:\n",
    "            break\n",
    "\n",
    "# â”€â”€ 4) ê²°ê³¼ ì¶œë ¥\n",
    "if titles:\n",
    "    print(\"\\nğŸ” ì¡ì½”ë¦¬ì•„ ì±„ìš© ê³µê³  (ìƒìœ„ 5ê°œ):\")\n",
    "    for i, (t, u) in enumerate(titles, 1):\n",
    "        print(f\"{i}. {t}\\n   {u}\\n\")\n",
    "else:\n",
    "    print(\"\\n! WARNING: í•„í„°ë§ëœ ê³µê³ ë¥¼ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "823646a6-932d-4ba3-8f82-2d5833403c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”:  SKí•˜ì´ë‹‰ìŠ¤\n",
      "í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ì¡ì½”ë¦¬ì•„ ì±„ìš© ê³µê³  (ì´ 32ê±´):\n",
      "1. SKí•˜ì´ë‹‰ìŠ¤ãˆœ\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47285675?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=1&sc=632\n",
      "\n",
      "2. SKí•˜ì´ë‹‰ìŠ¤ãˆœ\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47333539?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=2&sc=632\n",
      "\n",
      "3. SKí•˜ì´ë‹‰ìŠ¤ãˆœ\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47215809?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=3&sc=632\n",
      "\n",
      "4. ãˆœì¡°ì–‘í…Œí¬\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47305752?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=5&sc=631\n",
      "\n",
      "5. íƒ‘ì´ì•¤ì”¨ãˆœ\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47358409?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=7&sc=631\n",
      "\n",
      "6. ì‚¼í˜¸ê±´ì˜ãˆœ\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47237204?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=8&sc=631\n",
      "\n",
      "7. ãˆœë°œë ‰ìŠ¤ì„œë¹„ìŠ¤\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47258372?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=9&sc=631\n",
      "\n",
      "8. ì—ìŠ¤ì¼€ì´íƒ‘ãˆœ\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47358396?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=10&sc=631\n",
      "\n",
      "9. ãˆœì—”ì‹œìŠ¤í…œ\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47324924?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=11&sc=631\n",
      "\n",
      "10. ãˆœë°œë ‰ìŠ¤ì„œë¹„ìŠ¤\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47346210?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=12&sc=631\n",
      "\n",
      "11. ì”¨ì•¤í† íŠ¸í”ŒëŸ¬ìŠ¤ãˆœ\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47351532?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=13&sc=631\n",
      "\n",
      "12. ì”¨ì•¤í† íŠ¸í”ŒëŸ¬ìŠ¤ãˆœ\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47356754?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=14&sc=631\n",
      "\n",
      "13. ë” ì¼€ì´í…ãˆœ\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47349614?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=15&sc=631\n",
      "\n",
      "14. ãˆœë°œë ‰ìŠ¤ì„œë¹„ìŠ¤\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47285473?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=17&sc=631\n",
      "\n",
      "15. ãˆœìœ ë‹ˆí¬ìœ \n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47366978?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=18&sc=631\n",
      "\n",
      "16. ãˆœí•œìš¸ì•„ì´ì—”ì§€í™€ë”©ìŠ¤\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47121842?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=19&sc=631\n",
      "\n",
      "17. ãˆœë°œë ‰ìŠ¤ì„œë¹„ìŠ¤\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47280044?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=20&sc=631\n",
      "\n",
      "18. ì—ì´ìŠ¤íœ´ë¨¼íŒŒì›Œãˆœ\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47369724?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=23&sc=631\n",
      "\n",
      "19. ãˆœë°œë ‰ìŠ¤ì„œë¹„ìŠ¤\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47297648?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=24&sc=631\n",
      "\n",
      "20. ãˆœë¹„ë¹„ì†Œí”„íŠ¸ (BB Soft\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47319890?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=26&sc=631\n",
      "\n",
      "21. ãˆœì•„íë¼ì‹œìŠ¤í…œ\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47323027?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=27&sc=631\n",
      "\n",
      "22. ì—ìŠ¤ì¼€ì´ì‰´ë”ìŠ¤ãˆœ\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47383259?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=28&sc=631\n",
      "\n",
      "23. ãˆœë°œë ‰ìŠ¤ì„œë¹„ìŠ¤\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47355647?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=29&sc=631\n",
      "\n",
      "24. ãˆœë¹„ë¹„ì†Œí”„íŠ¸ (BB Soft\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47319880?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=30&sc=631\n",
      "\n",
      "25. ãˆœí”¼ì—ìŠ¤í”¼\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47296418?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=31&sc=631\n",
      "\n",
      "26. ãˆœë°œë ‰ìŠ¤ì„œë¹„ìŠ¤\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47118887?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=33&sc=631\n",
      "\n",
      "27. ãˆœë°œë ‰ìŠ¤ì„œë¹„ìŠ¤\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47250503?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=34&sc=631\n",
      "\n",
      "28. ãˆœë°œë ‰ìŠ¤ì„œë¹„ìŠ¤\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47369071?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=36&sc=631\n",
      "\n",
      "29. ãˆœë°œë ‰ìŠ¤ì„œë¹„ìŠ¤\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47191244?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=37&sc=631\n",
      "\n",
      "30. ãˆœë°œë ‰ìŠ¤ì„œë¹„ìŠ¤\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47346593?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=38&sc=631\n",
      "\n",
      "31. ãˆœì•„ìŠ¤íƒ€ì•„ì´ë¹„ì—ìŠ¤\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47203714?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=39&sc=631\n",
      "\n",
      "32. ãˆœìœ ë² ì´ìŠ¤\n",
      "   https://www.jobkorea.co.kr/Recruit/GI_Read/47176177?Oem_Code=C1&logpath=1&stext=SKí•˜ì´ë‹‰ìŠ¤&listno=40&sc=631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "\n",
    "# â”€â”€ ì‚¬ìš©ì ì…ë ¥\n",
    "keyword = input('í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ')\n",
    "page_num = int(input('í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”: '))  # ìˆ«ìë¡œ ë°”ë¡œ ë³€í™˜\n",
    "base_url = \"https://www.jobkorea.co.kr\"\n",
    "\n",
    "# â”€â”€ ìš”ì²­ í—¤ë”\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# â”€â”€ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸/ì§‘í•©ì€ ë£¨í”„ ë°–ì—ì„œ ì„ ì–¸!\n",
    "titles = []\n",
    "seen = set()\n",
    "\n",
    "for n in range(1, page_num + 1):           # 1ë¶€í„° page_numê¹Œì§€\n",
    "    search_url = (\n",
    "        f\"{base_url}/Search/?\"\n",
    "        f\"stext={urllib.parse.quote(keyword)}&\"\n",
    "        f\"tabType=recruit&Page_No={n}\"\n",
    "    )\n",
    "    try:\n",
    "        resp = requests.get(search_url, headers=HEADERS, timeout=10)\n",
    "        # HTTP ìš”ì²­ì„ ë³´ë‚¸ ë’¤ ì‘ë‹µ ì½”ë“œê°€ 200ë²ˆëŒ€(ì„±ê³µ)ì´ ì•„ë‹Œ ê²½ìš°ì— ì¦‰ì‹œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œì¤Œ.\n",
    "        # ë§Œì•½ 404ì¸ ì—ëŸ¬ê°€ í„°ì¡Œì„ê²½ìš° ë°”ë¡œ ì•„ë˜ì— ìˆëŠ” exceptë¸”ë¡ì—ì„œ ì¡ì•„ì„œ ì²˜ë¦¬í•¨.\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(f\"[í˜ì´ì§€ {n}] ìš”ì²­ ì‹¤íŒ¨:\", e)\n",
    "        continue\n",
    "\n",
    "    # â”€â”€ a íƒœê·¸ ì¤‘ ì±„ìš©ê³µê³  ë§í¬ë§Œ ê³¨ë¼ì„œ\n",
    "    for a in soup.select(\"a[href*='/Recruit/GI_Read/']\"):\n",
    "        href = a[\"href\"]\n",
    "        full_url = urllib.parse.urljoin(base_url, href)\n",
    "\n",
    "        if full_url in seen:\n",
    "            continue\n",
    "        seen.add(full_url)\n",
    "\n",
    "        # â”€â”€ <a> ë°”ë¡œ ì•„ë˜ <span> ì— ì œëª©ì´ ê°ì‹¸ì—¬ ìˆë‹¤ë©´\n",
    "        span = a.find(\"span\")\n",
    "        if not span:\n",
    "            continue  # spanì´ ì—†ìœ¼ë©´ ë‹¤ìŒ ë§í¬ë¡œ\n",
    "\n",
    "        title = span.get_text(strip=True)\n",
    "        if len(title) < 5:\n",
    "            continue\n",
    "\n",
    "        titles.append((title, full_url))\n",
    "\n",
    "# â”€â”€ ì¶œë ¥\n",
    "print(f\"ğŸ” ì¡ì½”ë¦¬ì•„ ì±„ìš© ê³µê³  (ì´ {len(titles)}ê±´):\")\n",
    "for i, (t, u) in enumerate(titles, 1):\n",
    "    print(f\"{i}. {t}\\n   {u}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13b277c9-ba00-41c8-a6f8-d3c6eef1b3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íšŒì‚¬ëª…: ì¶”ì¶œ ì‹¤íŒ¨\n",
      "ê³µê³  ì œëª©: ì¶”ì¶œ ì‹¤íŒ¨\n",
      "ì§€ì›ìê²© ìƒì„¸:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url = (\n",
    "    \"https://www.jobkorea.co.kr/Recruit/GI_Read/\"\n",
    "    \"47370797?Oem_Code=C1&logpath=1&\"\n",
    "    \"stext=%ED%8C%8C%EC%9D%B4%EC%8D%AC&\"\n",
    "    \"listno=2&sc=631\"\n",
    ")\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "resp = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "# 1) íšŒì‚¬ëª… ì¶”ì¶œ: í´ë˜ìŠ¤ëª…ì— 'coName'ì´ í¬í•¨ëœ span\n",
    "co_tag = soup.find(\"span\", class_=re.compile(r\"coName\"))\n",
    "company_name = co_tag.get_text(strip=True) if co_tag else \"ì¶”ì¶œ ì‹¤íŒ¨\"\n",
    "print(\"íšŒì‚¬ëª…:\", company_name)\n",
    "\n",
    "# 2) ê³µê³  ì œëª© ì¶”ì¶œ:\n",
    "#    h3 íƒœê·¸ ì¤‘ í´ë˜ìŠ¤ì— 'hd_'ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒ ì°¾ê¸°\n",
    "h3 = soup.find(\"h3\", class_=re.compile(r\"hd_\"))\n",
    "job_title = None\n",
    "if h3:\n",
    "    # h3íƒœê·¸ ë‚´ë¶€ì— ìˆëŠ” ëª¨ë“  spaníƒœê·¸ ì œê±°\n",
    "    for span in h3.find_all(\"span\"):\n",
    "        span.extract()\n",
    "    # h3íƒœê·¸ ë‚´ë¶€ì— ìˆëŠ” píƒœê·¸ì¤‘ classê°€ 'txt'ì¸ì• ë“¤ë§Œ ì œê±°\n",
    "    for p_txt in h3.find_all(\"p\", class_=re.compile(r\"txt\")):\n",
    "        p_txt.extract()\n",
    "    # ë‚¨ì€ í…ìŠ¤íŠ¸ê°€ ì œëª©\n",
    "    job_title = h3.get_text(strip=True)\n",
    "print(\"ê³µê³  ì œëª©:\", job_title or \"ì¶”ì¶œ ì‹¤íŒ¨\")\n",
    "\n",
    "# 4) ìƒì„¸ ì§€ì›ìê²©(ê²½ë ¥, í•™ë ¥, ìŠ¤í‚¬) ì¶”ì¶œ\n",
    "qual_details = {}\n",
    "# tbRow clear í´ë˜ìŠ¤ ê°€ì§„ div ì „ë¶€ ìˆœíšŒ\n",
    "for div in soup.find_all(\"div\", class_=re.compile(r\"tbRow.*clear\")):\n",
    "    h4 = div.find(\"h4\")\n",
    "    # h4 í…ìŠ¤íŠ¸ì— 'ì§€ì›ìê²©'ì´ í¬í•¨ëœ ë¸”ë¡ë§Œ ì„ íƒ\n",
    "    if h4 and \"ì§€ì›ìê²©\" in h4.get_text():\n",
    "        dl = div.find(\"dl\", class_=\"tbList\")\n",
    "        if dl:\n",
    "            # dt/dd ìŒìœ¼ë¡œ ìˆœíšŒí•˜ë©° ë”•ì…”ë„ˆë¦¬ì— ì €ì¥\n",
    "            for dt, dd in zip(dl.find_all(\"dt\"), dl.find_all(\"dd\")):\n",
    "                key = dt.get_text(strip=True)\n",
    "                val = dd.get_text(strip=True)\n",
    "                qual_details[key] = val\n",
    "        break\n",
    "\n",
    "qual = extract_section(\"ì§€ì›ìê²©\")\n",
    "work = extract_section(\"ê·¼ë¬´ì¡°ê±´\")\n",
    "print(\"ì§€ì›ìê²© ìƒì„¸:\")\n",
    "for k, v in qual_details.items():\n",
    "    print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fc2f2a9-879b-4e18-9bed-d50f705d2b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” HTTP ìƒíƒœ ì½”ë“œ: 200\n",
      "\n",
      "â”€â”€ HTML ì²˜ìŒ 500ì â”€â”€\n",
      " \t\thtml, body, div, p, h1, img =\"width=device-width, initial-scale=1.0, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0\">\t\n",
      "â€¦\n",
      "\n",
      "â”€â”€ ALL <span> TAGS & CLASSES â”€â”€\n",
      "span: 'ì´ìš©ì´ ì¼ì‹œì ìœ¼ë¡œ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.' | class: None\n",
      "span: 'ë³´ì•ˆë¬¸ìë¥¼ ì…ë ¥í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.' | class: None\n",
      "span: 'TEL :' | class: ['bold']\n",
      "span: '1588-9350' | class: None\n",
      "span: 'e-ë©”ì¼ :' | class: ['bold']\n",
      "span: 'helpdesk@jobkorea.co.kr' | class: None\n",
      "span: 'ì „í™”ìƒë‹´ì‹œê°„ :' | class: ['bold']\n",
      "span: '[ì›”-ê¸ˆ] 09:00~19:00 [í† ] 09:00~15:00' | class: None\n",
      "\n",
      "â”€â”€ FIND span[class*=coName] â”€â”€\n",
      "\n",
      "â”€â”€ ALL <h3> TAGS & CLASSES â”€â”€\n",
      "\n",
      "â”€â”€ FIND h3[class^=hd_] â”€â”€\n",
      "\n",
      "â”€â”€ FIND div[class*=tbRow] â”€â”€\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url = (\n",
    "    \"https://www.jobkorea.co.kr/Recruit/GI_Read/\"\n",
    "    \"47370797?Oem_Code=C1&logpath=1&\"\n",
    "    \"stext=%ED%8C%8C%EC%9D%B4%EC%8D%AC&\"\n",
    "    \"listno=2&sc=631\"\n",
    ")\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "resp = requests.get(url, headers=headers)\n",
    "print(\"ğŸ” HTTP ìƒíƒœ ì½”ë“œ:\", resp.status_code)\n",
    "\n",
    "html = resp.text\n",
    "print(\"\\nâ”€â”€ HTML ì²˜ìŒ 500ì â”€â”€\")\n",
    "print(html[:500].replace(\"\\n\", \" \"), \"\\nâ€¦\\n\")\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 1) ëª¨ë“  span íƒœê·¸ì™€ í´ë˜ìŠ¤ ì°ì–´ë³´ê¸°\n",
    "print(\"â”€â”€ ALL <span> TAGS & CLASSES â”€â”€\")\n",
    "for span in soup.find_all(\"span\"):\n",
    "    print(\"span:\", repr(span.get_text(strip=True)), \"| class:\", span.get(\"class\"))\n",
    "print()\n",
    "\n",
    "# 2) 'coName' ë°©ì‹ìœ¼ë¡œ ì°¾ëŠ” span íƒœê·¸ ì°ì–´ë³´ê¸°\n",
    "print(\"â”€â”€ FIND span[class*=coName] â”€â”€\")\n",
    "for span in soup.find_all(\"span\", class_=re.compile(r\"coName\")):\n",
    "    print(span)\n",
    "print()\n",
    "\n",
    "# 3) ëª¨ë“  h3 íƒœê·¸ì™€ í´ë˜ìŠ¤ ì°ì–´ë³´ê¸°\n",
    "print(\"â”€â”€ ALL <h3> TAGS & CLASSES â”€â”€\")\n",
    "for h3 in soup.find_all(\"h3\"):\n",
    "    print(\"h3 class:\", h3.get(\"class\"), \"| html:\", h3.prettify())\n",
    "print()\n",
    "\n",
    "# 4) 'hd_' íŒ¨í„´ìœ¼ë¡œ ì°¾ëŠ” h3 íƒœê·¸ ì°ì–´ë³´ê¸°\n",
    "print(\"â”€â”€ FIND h3[class^=hd_] â”€â”€\")\n",
    "for h3 in soup.find_all(\"h3\", class_=re.compile(r\"hd_\")):\n",
    "    print(h3.prettify())\n",
    "print()\n",
    "\n",
    "# 5) tbRow clear ë¸”ë¡ ì°ì–´ë³´ê¸°\n",
    "print(\"â”€â”€ FIND div[class*=tbRow] â”€â”€\")\n",
    "for div in soup.find_all(\"div\", class_=re.compile(r\"tbRow\")):\n",
    "    h4 = div.find(\"h4\")\n",
    "    print(\"BLOCK h4 text:\", h4.get_text(strip=True) if h4 else \"<no h4>\")\n",
    "    print(div.prettify(), \"\\n---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "179ce7b0-8ff8-437f-a190-59ea5d30f579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”:  í† ìŠ¤\n",
      "í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì™„ë£Œ: jobs.txt íŒŒì¼ì— 20ê±´ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import re\n",
    "\n",
    "# â”€â”€ ì„¤ì •\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "BASE_URL = \"https://www.jobkorea.co.kr\"\n",
    "\n",
    "def get_job_links(keyword, page_num):\n",
    "    links = []\n",
    "    seen = set()\n",
    "    for n in range(1, page_num + 1):\n",
    "        search_url = (\n",
    "            f\"{BASE_URL}/Search/?\"\n",
    "            f\"stext={urllib.parse.quote(keyword)}&\"\n",
    "            f\"tabType=recruit&Page_No={n}\"\n",
    "        )\n",
    "        try:\n",
    "            resp = requests.get(search_url, headers=HEADERS, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"[í˜ì´ì§€ {n}] ìš”ì²­ ì‹¤íŒ¨:\", e)\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        for a in soup.select(\"a[href*='/Recruit/GI_Read/']\"):\n",
    "            href = a[\"href\"]\n",
    "            full_url = urllib.parse.urljoin(BASE_URL, href)\n",
    "            if full_url in seen:\n",
    "                continue\n",
    "            seen.add(full_url)\n",
    "\n",
    "            span = a.find(\"span\")\n",
    "            if not span:\n",
    "                continue\n",
    "            title = span.get_text(strip=True)\n",
    "            if len(title) < 5:\n",
    "                continue\n",
    "\n",
    "            links.append((title, full_url))\n",
    "    return links\n",
    "\n",
    "def parse_job_detail(url):\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"[ìƒì„¸] {url} ìš”ì²­ ì‹¤íŒ¨:\", e)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    co_tag = soup.find(\"span\", class_=re.compile(r\"coName\"))\n",
    "    company_name = co_tag.get_text(strip=True) if co_tag else \"\"\n",
    "\n",
    "    h3 = soup.find(\"h3\", class_=re.compile(r\"hd_\"))\n",
    "    job_title = \"\"\n",
    "    if h3:\n",
    "        for span in h3.find_all(\"span\"):\n",
    "            span.extract()\n",
    "        for p_txt in h3.find_all(\"p\", class_=re.compile(r\"txt\")):\n",
    "            p_txt.extract()\n",
    "        job_title = h3.get_text(strip=True)\n",
    "\n",
    "    def extract_section(title):\n",
    "        tit = soup.find(\"div\", string=re.compile(title))\n",
    "        if tit:\n",
    "            content = tit.find_next_sibling(\"div\")\n",
    "            return content.get_text(strip=True) if content else \"\"\n",
    "        return \"\"\n",
    "    simple_qual = extract_section(\"ì§€ì›ìê²©\")\n",
    "    work = extract_section(\"ê·¼ë¬´ì¡°ê±´\")\n",
    "\n",
    "    qual_details = {}\n",
    "    for div in soup.find_all(\"div\", class_=re.compile(r\"tbRow.*clear\")):\n",
    "        h4 = div.find(\"h4\")\n",
    "        if h4 and \"ì§€ì›ìê²©\" in h4.get_text():\n",
    "            dl = div.find(\"dl\", class_=\"tbList\")\n",
    "            if dl:\n",
    "                for dt, dd in zip(dl.find_all(\"dt\"), dl.find_all(\"dd\")):\n",
    "                    qual_details[dt.get_text(strip=True)] = dd.get_text(strip=True)\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"company_name\": company_name,\n",
    "        \"job_title\": job_title,\n",
    "        \"simple_qual\": simple_qual,\n",
    "        \"work\": work,\n",
    "        \"qual_details\": qual_details\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    keyword = input(\"í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "    page_num = int(input(\"í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”: \"))\n",
    "    links = get_job_links(keyword, page_num)\n",
    "\n",
    "    with open(\"jobKorea.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, (_, url) in enumerate(links, 1):\n",
    "            f.write(f\"=== ê³µê³  {idx} ===\\n\")\n",
    "\n",
    "            data = parse_job_detail(url)\n",
    "            if not data:\n",
    "                f.write(\"ìƒì„¸ ì •ë³´ íŒŒì‹± ì‹¤íŒ¨\\n\\n\")\n",
    "                continue\n",
    "\n",
    "            f.write(f\"íšŒì‚¬ëª…: {data['company_name']}\\n\")\n",
    "            f.write(f\"ê³µê³  ì œëª©: {data['job_title']}\\n\")\n",
    "            f.write(f\"ì§€ì›ìê²© (ìš”ì•½): {data['simple_qual']}\\n\")\n",
    "            f.write(f\"ê·¼ë¬´ì¡°ê±´: {data['work']}\\n\")\n",
    "            f.write(\"ì§€ì›ìê²© ìƒì„¸:\\n\")\n",
    "            for k, v in data[\"qual_details\"].items():\n",
    "                f.write(f\"  - {k}: {v}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"ì™„ë£Œ: jobKorea.txt íŒŒì¼ì— {len(links)}ê±´ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04ecb6-be02-416b-8b54-e605e3e54c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

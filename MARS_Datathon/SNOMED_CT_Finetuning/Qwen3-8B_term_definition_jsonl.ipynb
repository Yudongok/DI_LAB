{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyODg4VACgSCfNSFLinzk1X5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"OydcirndbIwO"},"outputs":[],"source":["from google.colab import drive\n","\n","# 구글 드라이브 마운트\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install -U \"transformers>=4.46.0\" \"trl==0.9.6\" \"peft>=0.13.0\" \"accelerate>=0.34.2\" \"bitsandbytes>=0.43.3\"\n"],"metadata":{"id":"O2coJr2ZbT7L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Fine-tune Qwen3-8B with SNOMED term-definition JSONL (messages format)\n","- QLoRA (4bit) + PEFT (LoRA)\n","- TRL SFTTrainer로 chat template 적용\n","- 진행상황 표시: tqdm 진행바 + 스텝별 콘솔 로그 + TensorBoard 로깅\n","\"\"\"\n","\n","import os, json\n","from dataclasses import dataclass\n","from typing import Dict, List, Any\n","\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    TrainerCallback,            # ✅ 추가: 콘솔 로그 콜백용\n",")\n","from trl import SFTTrainer\n","from peft import LoraConfig\n","\n","# ============== 사용자 설정 ==============\n","MODEL_NAME = \"/content/drive/MyDrive/DILAB/qwen3-8b\"   # 또는 \"Qwen/Qwen2.5-8B-Instruct\" 등\n","DATA_JSONL = \"/content/drive/MyDrive/DILAB/OK/DI_LAB/MARS_Datathon/Datasets/SNOMED_CT_datasets/snomed_term_definition_only.jsonl\"\n","\n","OUTPUT_DIR = \"/content/drive/MyDrive/DILAB/OK/DI_LAB/MARS_Datathon/Models/qwen3_8b_snomed_lora\"\n","MICRO_BATCH = 4\n","GRAD_ACCUM = 8                  # 유효 배치 = MICRO_BATCH * GRAD_ACCUM\n","LR = 2e-5\n","EPOCHS = 2\n","MAX_SEQ_LEN = 1024\n","USE_FLASH_ATTN = False\n","BF16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n","\n","# TensorBoard 로그 경로 (OUTPUT_DIR 하위에 저장)\n","TB_LOGDIR = os.path.join(OUTPUT_DIR, \"tb_logs\")\n","# ========================================\n","\n","def get_tokenizer(model_name: str):\n","    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=True)\n","    if tok.pad_token is None:\n","        tok.pad_token = tok.eos_token\n","    return tok\n","\n","def format_with_chat_template(tokenizer, messages: List[Dict[str, str]]) -> str:\n","    return tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False,\n","        add_generation_prompt=False,\n","        truncation=True\n","    )\n","\n","def make_dataset(tokenizer, data_path: str):\n","    ds = load_dataset(\"json\", data_files=data_path, split=\"train\")\n","    def map_fn(ex):\n","        msgs = ex[\"messages\"]\n","        text = format_with_chat_template(tokenizer, msgs)\n","        return {\"text\": text}\n","    ds = ds.map(map_fn, remove_columns=ds.column_names)\n","    return ds\n","\n","# ✅ 스텝별로 콘솔에 손실/학습률을 출력하는 콜백\n","class LossPrinterCallback(TrainerCallback):\n","    def on_log(self, args, state, control, logs=None, **kwargs):\n","        if not logs:\n","            return\n","        # 주요 지표만 선별 출력\n","        keys = [\"loss\", \"learning_rate\", \"grad_norm\", \"epoch\"]\n","        msg = \" | \".join([f\"{k}: {logs[k]:.6f}\" for k in keys if k in logs])\n","        if msg:\n","            print(f\"[step {state.global_step}] {msg}\")\n","\n","def main():\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_compute_dtype=torch.bfloat16 if BF16 else torch.float16\n","    )\n","\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_NAME,\n","        trust_remote_code=True,\n","        quantization_config=bnb_config,\n","        attn_implementation=\"flash_attention_2\" if USE_FLASH_ATTN else \"eager\",\n","        torch_dtype=torch.bfloat16 if BF16 else torch.float16,\n","        device_map=\"auto\",\n","    )\n","    tok = get_tokenizer(MODEL_NAME)\n","\n","    lora_cfg = LoraConfig(\n","        r=32,\n","        lora_alpha=64,\n","        lora_dropout=0.05,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n","    )\n","\n","    train_ds = make_dataset(tok, DATA_JSONL)\n","\n","    args = TrainingArguments(\n","        output_dir=OUTPUT_DIR,\n","        num_train_epochs=EPOCHS,\n","        per_device_train_batch_size=MICRO_BATCH,\n","        gradient_accumulation_steps=GRAD_ACCUM,\n","        learning_rate=LR,\n","        lr_scheduler_type=\"cosine\",\n","        warmup_ratio=0.05,\n","\n","        # ✅ 진행바/로그 관련\n","        logging_steps=10,                   # 10스텝마다 로그 이벤트 발생\n","        disable_tqdm=False,                 # tqdm 진행바 활성화\n","        report_to=[\"tensorboard\"],          # ✅ TensorBoard 로깅\n","        logging_dir=TB_LOGDIR,              # ✅ 로그 저장 폴더\n","\n","        save_steps=1000,\n","        save_total_limit=2,\n","\n","        bf16=BF16,\n","        fp16=not BF16,\n","        optim=\"paged_adamw_32bit\",\n","        gradient_checkpointing=True,\n","        max_grad_norm=1.0,\n","    )\n","\n","    trainer = SFTTrainer(\n","        model=model,\n","        tokenizer=tok,\n","        peft_config=lora_cfg,\n","        train_dataset=train_ds,\n","        dataset_text_field=\"text\",\n","        max_seq_length=MAX_SEQ_LEN,\n","        packing=True,\n","        args=args,\n","        callbacks=[LossPrinterCallback()],   # ✅ 콘솔 로그 콜백 추가\n","    )\n","\n","    trainer.train()\n","    trainer.save_model()\n","    tok.save_pretrained(OUTPUT_DIR)\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"bzbx7__6bvfw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 용어 정의 제대로 하는지 검증(영어 출력 버전)"],"metadata":{"id":"ZU6m0bFS6kV_"}},{"cell_type":"code","source":["# Inference: Ask the fine-tuned model to define a term\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from peft import PeftModel\n","import torch\n","\n","# ==== 경로 설정 ====\n","BASE_MODEL   = \"/content/drive/MyDrive/DILAB/qwen3-8b\"  # 베이스 모델(또는 허깅페이스 경로)\n","ADAPTER_DIR  = \"/content/drive/MyDrive/DILAB/OK/DI_LAB/MARS_Datathon/Models/qwen3_8b_snomed_lora\"  # LoRA 어댑터\n","USE_4BIT     = True\n","USE_BF16     = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n","\n","# ==== 모델/토크나이저 로드 ====\n","bnb = None\n","if USE_4BIT:\n","    bnb = BitsAndBytesConfig(\n","        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_compute_dtype=torch.bfloat16 if USE_BF16 else torch.float16\n","    )\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    BASE_MODEL,\n","    quantization_config=bnb,\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n","    dtype=torch.bfloat16 if USE_BF16 else torch.float16,  # torch_dtype deprec → dtype\n",")\n","tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, trust_remote_code=True)\n","if tok.pad_token is None:\n","    tok.pad_token = tok.eos_token\n","\n","# LoRA 어댑터 부착 (병합 모델이면 이 부분 건너뜀)\n","model = PeftModel.from_pretrained(model, ADAPTER_DIR)\n","model.eval()\n","\n","def define_term_en(term: str, max_new_tokens: int = 120, deterministic: bool = True) -> str:\n","    \"\"\"\n","    Return a concise English definition (1–2 sentences) for a medical term.\n","    - Strong system prompt forces brevity & English.\n","    - Slice by token length to avoid prompt-bleed.\n","    \"\"\"\n","    messages = [\n","        {\"role\": \"system\", \"content\":\n","         \"You are a clinical assistant. Provide a concise, accurate definition in English, \"\n","         \"limited to 1–2 sentences. Avoid preambles or meta commentary.\"},\n","        {\"role\": \"user\", \"content\": term}\n","    ]\n","\n","    # Build prompt with the model's chat template\n","    prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","    inputs = tok([prompt], return_tensors=\"pt\").to(model.device)\n","    with torch.no_grad():\n","        out = model.generate(\n","            **inputs,\n","            max_new_tokens=max_new_tokens,\n","            do_sample=not deterministic,   # for strict determinism set to False\n","            temperature=0.2,\n","            top_p=0.9,\n","            eos_token_id=tok.eos_token_id,\n","            pad_token_id=tok.pad_token_id\n","        )\n","\n","    # NEW: slice by token length (not by string length)\n","    gen_ids = out[0]\n","    new_ids = gen_ids[inputs[\"input_ids\"].shape[1]:]\n","    answer = tok.decode(new_ids, skip_special_tokens=True).strip()\n","\n","    # optional: ultra-concise post-trim (keep 2 sentences max)\n","    # import re\n","    # sents = re.split(r'(?<=[.!?])\\s+', answer)\n","    # answer = ' '.join(sents[:2]).strip()\n","\n","    return answer\n","\n","print(define_term_en(\"Asthma\"))\n","print(define_term_en(\"Myocardial infarction\"))\n","\n"],"metadata":{"id":"dNVF7uXXcdCe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 한국어 출력 버전"],"metadata":{"id":"UvQkYfeP9sjr"}},{"cell_type":"code","source":["# Inference: Ask the fine-tuned model to define a term\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from peft import PeftModel\n","import torch\n","\n","# ==== 경로 설정 ====\n","BASE_MODEL   = \"/content/drive/MyDrive/DILAB/qwen3-8b\"  # 베이스 모델(또는 허깅페이스 경로)\n","ADAPTER_DIR  = \"/content/drive/MyDrive/DILAB/OK/DI_LAB/MARS_Datathon/Models/qwen3_8b_snomed_lora\"  # LoRA 어댑터\n","USE_4BIT     = True\n","USE_BF16     = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n","\n","# ==== 모델/토크나이저 로드 ====\n","bnb = None\n","if USE_4BIT:\n","    bnb = BitsAndBytesConfig(\n","        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_compute_dtype=torch.bfloat16 if USE_BF16 else torch.float16\n","    )\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    BASE_MODEL,\n","    quantization_config=bnb,\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n","    dtype=torch.bfloat16 if USE_BF16 else torch.float16,  # torch_dtype deprec → dtype\n",")\n","tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, trust_remote_code=True)\n","if tok.pad_token is None:\n","    tok.pad_token = tok.eos_token\n","\n","# LoRA 어댑터 부착 (병합 모델이면 이 부분 건너뜀)\n","model = PeftModel.from_pretrained(model, ADAPTER_DIR)\n","model.eval()\n","\n","def define_term_ko(term: str, max_new_tokens: int = 200, deterministic: bool = True) -> str:\n","    # 1) 한국어 지시를 system에 명시\n","    messages = [\n","        {\"role\": \"system\", \"content\":\n","         \"당신은 임상 지식을 가진 의료 보조자입니다. \"\n","         \"사용자가 제시한 의학 용어를 한국어로 간결하고 정확하게 정의하세요. \"\n","         \"불필요한 서론/메모/추측은 금지하고 1~3문장으로 답하세요.\"},\n","        {\"role\": \"user\", \"content\": term}\n","    ]\n","    prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","    # 2) 토큰화\n","    inputs = tok([prompt], return_tensors=\"pt\").to(model.device)\n","\n","    # 3) 생성\n","    with torch.no_grad():\n","        out = model.generate(\n","            **inputs,\n","            max_new_tokens=max_new_tokens,\n","            do_sample=not deterministic,   # 정의문은 보통 결정적 생성 권장\n","            temperature=0.2,\n","            top_p=0.9,\n","            eos_token_id=tok.eos_token_id,\n","            pad_token_id=tok.pad_token_id\n","        )\n","\n","    # 4) \"토큰 길이\" 기준으로 신규 토큰만 추출 → 디코드\n","    gen_ids = out[0]\n","    new_token_ids = gen_ids[inputs[\"input_ids\"].shape[1]:]  # ← 핵심!\n","    answer = tok.decode(new_token_ids, skip_special_tokens=True).strip()\n","    return answer\n","\n","# === 사용 예시 ===\n","print(define_term_ko(\"Asthma\"))\n","print(define_term_ko(\"Myocardial infarction\"))\n"],"metadata":{"id":"QfQdsy8A6ui9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hGITGSGd92CF"},"execution_count":null,"outputs":[]}]}
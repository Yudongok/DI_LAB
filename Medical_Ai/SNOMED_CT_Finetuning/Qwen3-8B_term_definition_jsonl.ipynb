{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyPdHLd3+LyjIoKaRIz0PX6u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"bb3243ccb03d4a72898be90b9fade88e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_16541ea085ff45d3848d74857d7ee45e","IPY_MODEL_d7c78d5425c84962a89afbcce6bc36da","IPY_MODEL_796694ce76a14d248283047d39d105e1"],"layout":"IPY_MODEL_43e9c2fd62b54ab798645a9f60c0fabb"}},"16541ea085ff45d3848d74857d7ee45e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4df4227dfe284ed59351b80427bc70cd","placeholder":"​","style":"IPY_MODEL_b336a83b5b7143ebad9bd1643070601b","value":"Loading checkpoint shards: 100%"}},"d7c78d5425c84962a89afbcce6bc36da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ec9fe7fda884e1088a6f113d0938e14","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dde33462c3e64cde967038fdfd4a5912","value":5}},"796694ce76a14d248283047d39d105e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4e17dc0cd154a2bb80d00d3bc64f377","placeholder":"​","style":"IPY_MODEL_0703501c769e4650ae61162866fc36a5","value":" 5/5 [00:24&lt;00:00,  4.03s/it]"}},"43e9c2fd62b54ab798645a9f60c0fabb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4df4227dfe284ed59351b80427bc70cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b336a83b5b7143ebad9bd1643070601b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ec9fe7fda884e1088a6f113d0938e14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dde33462c3e64cde967038fdfd4a5912":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4e17dc0cd154a2bb80d00d3bc64f377":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0703501c769e4650ae61162866fc36a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"OydcirndbIwO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760395320945,"user_tz":-540,"elapsed":1930,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"18e5c05b-a30c-4c47-a0d7-b8bcbd7767d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","\n","# 구글 드라이브 마운트\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install -U \"transformers>=4.46.0\" \"trl==0.9.6\" \"peft>=0.13.0\" \"accelerate>=0.34.2\" \"bitsandbytes>=0.43.3\"\n"],"metadata":{"id":"O2coJr2ZbT7L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760395332577,"user_tz":-540,"elapsed":4987,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"793e1452-b900-4c46-b427-a503d0ad36bc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.12/dist-packages (4.57.0)\n","Requirement already satisfied: trl==0.9.6 in /usr/local/lib/python3.12/dist-packages (0.9.6)\n","Requirement already satisfied: peft>=0.13.0 in /usr/local/lib/python3.12/dist-packages (0.17.1)\n","Requirement already satisfied: accelerate>=0.34.2 in /usr/local/lib/python3.12/dist-packages (1.10.1)\n","Requirement already satisfied: bitsandbytes>=0.43.3 in /usr/local/lib/python3.12/dist-packages (0.48.1)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (2.8.0+cu126)\n","Requirement already satisfied: numpy<2.0.0,>=1.18.2 in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (1.26.4)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (4.0.0)\n","Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (0.9.35)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (3.20.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (0.35.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (0.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft>=0.13.0) (5.9.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (1.1.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.4.0)\n","Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.9.6) (0.17.0)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.9.6) (13.9.4)\n","Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.9.6) (1.7.2)\n","Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.9.6) (4.4.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (0.70.16)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (2025.10.5)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (3.13.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (2.19.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4.0->trl==0.9.6) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4.0->trl==0.9.6) (3.0.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.9.6) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.9.6) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.9.6) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.9.6) (1.22.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.9.6) (1.17.0)\n"]}]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Fine-tune Qwen3-8B with SNOMED term-definition JSONL (messages format)\n","- QLoRA (4bit) + PEFT (LoRA)\n","- TRL SFTTrainer로 chat template 적용\n","- 진행상황 표시: tqdm 진행바 + 스텝별 콘솔 로그 + TensorBoard 로깅\n","\"\"\"\n","\n","import os, json\n","from dataclasses import dataclass\n","from typing import Dict, List, Any\n","\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    TrainerCallback,            # ✅ 추가: 콘솔 로그 콜백용\n",")\n","from trl import SFTTrainer\n","from peft import LoraConfig\n","\n","# ============== 사용자 설정 ==============\n","MODEL_NAME = \"/content/drive/MyDrive/DILAB/qwen3-8b\"   # 또는 \"Qwen/Qwen2.5-8B-Instruct\" 등\n","DATA_JSONL = \"/content/drive/MyDrive/DILAB/OK/DI_LAB/MARS_Datathon/Datasets/SNOMED_CT_datasets/snomed_term_definition_only.jsonl\"\n","\n","OUTPUT_DIR = \"/content/drive/MyDrive/DILAB/OK/DI_LAB/MARS_Datathon/Models/qwen3_8b_snomed_lora\"\n","MICRO_BATCH = 4\n","GRAD_ACCUM = 8                  # 유효 배치 = MICRO_BATCH * GRAD_ACCUM\n","LR = 2e-5\n","EPOCHS = 2\n","MAX_SEQ_LEN = 1024\n","USE_FLASH_ATTN = False\n","BF16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n","\n","# TensorBoard 로그 경로 (OUTPUT_DIR 하위에 저장)\n","TB_LOGDIR = os.path.join(OUTPUT_DIR, \"tb_logs\")\n","# ========================================\n","\n","def get_tokenizer(model_name: str):\n","    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=True)\n","    if tok.pad_token is None:\n","        tok.pad_token = tok.eos_token\n","    return tok\n","\n","def format_with_chat_template(tokenizer, messages: List[Dict[str, str]]) -> str:\n","    return tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False,\n","        add_generation_prompt=False,\n","        truncation=True\n","    )\n","\n","def make_dataset(tokenizer, data_path: str):\n","    ds = load_dataset(\"json\", data_files=data_path, split=\"train\")\n","    def map_fn(ex):\n","        msgs = ex[\"messages\"]\n","        text = format_with_chat_template(tokenizer, msgs)\n","        return {\"text\": text}\n","    ds = ds.map(map_fn, remove_columns=ds.column_names)\n","    return ds\n","\n","# ✅ 스텝별로 콘솔에 손실/학습률을 출력하는 콜백\n","class LossPrinterCallback(TrainerCallback):\n","    def on_log(self, args, state, control, logs=None, **kwargs):\n","        if not logs:\n","            return\n","        # 주요 지표만 선별 출력\n","        keys = [\"loss\", \"learning_rate\", \"grad_norm\", \"epoch\"]\n","        msg = \" | \".join([f\"{k}: {logs[k]:.6f}\" for k in keys if k in logs])\n","        if msg:\n","            print(f\"[step {state.global_step}] {msg}\")\n","\n","def main():\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_compute_dtype=torch.bfloat16 if BF16 else torch.float16\n","    )\n","\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_NAME,\n","        trust_remote_code=True,\n","        quantization_config=bnb_config,\n","        attn_implementation=\"flash_attention_2\" if USE_FLASH_ATTN else \"eager\",\n","        torch_dtype=torch.bfloat16 if BF16 else torch.float16,\n","        device_map=\"auto\",\n","    )\n","    tok = get_tokenizer(MODEL_NAME)\n","\n","    lora_cfg = LoraConfig(\n","        r=32,\n","        lora_alpha=64,\n","        lora_dropout=0.05,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n","    )\n","\n","    train_ds = make_dataset(tok, DATA_JSONL)\n","\n","    args = TrainingArguments(\n","        output_dir=OUTPUT_DIR,\n","        num_train_epochs=EPOCHS,\n","        per_device_train_batch_size=MICRO_BATCH,\n","        gradient_accumulation_steps=GRAD_ACCUM,\n","        learning_rate=LR,\n","        lr_scheduler_type=\"cosine\",\n","        warmup_ratio=0.05,\n","\n","        # ✅ 진행바/로그 관련\n","        logging_steps=10,                   # 10스텝마다 로그 이벤트 발생\n","        disable_tqdm=False,                 # tqdm 진행바 활성화\n","        report_to=[\"tensorboard\"],          # ✅ TensorBoard 로깅\n","        logging_dir=TB_LOGDIR,              # ✅ 로그 저장 폴더\n","\n","        save_steps=1000,\n","        save_total_limit=2,\n","\n","        bf16=BF16,\n","        fp16=not BF16,\n","        optim=\"paged_adamw_32bit\",\n","        gradient_checkpointing=True,\n","        max_grad_norm=1.0,\n","    )\n","\n","    trainer = SFTTrainer(\n","        model=model,\n","        tokenizer=tok,\n","        peft_config=lora_cfg,\n","        train_dataset=train_ds,\n","        dataset_text_field=\"text\",\n","        max_seq_length=MAX_SEQ_LEN,\n","        packing=True,\n","        args=args,\n","        callbacks=[LossPrinterCallback()],   # ✅ 콘솔 로그 콜백 추가\n","    )\n","\n","    trainer.train()\n","    trainer.save_model()\n","    tok.save_pretrained(OUTPUT_DIR)\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"bzbx7__6bvfw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 용어 정의 제대로 하는지 검증(영어 출력 버전)"],"metadata":{"id":"ZU6m0bFS6kV_"}},{"cell_type":"code","source":["# Inference: Ask the fine-tuned model to define a term\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from peft import PeftModel\n","import torch\n","\n","# ==== 경로 설정 ====\n","BASE_MODEL   = \"/content/drive/MyDrive/DILAB/qwen3-8b\"  # 베이스 모델(또는 허깅페이스 경로)\n","ADAPTER_DIR  = \"/content/drive/MyDrive/DILAB/OK_/Models/qwen3_8b_snomed_lora\"  # LoRA 어댑터\n","USE_4BIT     = True\n","USE_BF16     = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n","\n","# ==== 모델/토크나이저 로드 ====\n","bnb = None\n","if USE_4BIT:\n","    bnb = BitsAndBytesConfig(\n","        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_compute_dtype=torch.bfloat16 if USE_BF16 else torch.float16\n","    )\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    BASE_MODEL,\n","    quantization_config=bnb,\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n","    dtype=torch.bfloat16 if USE_BF16 else torch.float16,  # torch_dtype deprec → dtype\n",")\n","tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, trust_remote_code=True)\n","if tok.pad_token is None:\n","    tok.pad_token = tok.eos_token\n","\n","# LoRA 어댑터 부착 (병합 모델이면 이 부분 건너뜀)\n","model = PeftModel.from_pretrained(model, ADAPTER_DIR)\n","model.eval()\n","\n","def define_term_en(term: str, max_new_tokens: int = 120, deterministic: bool = True) -> str:\n","    \"\"\"\n","    Return a concise English definition (1–2 sentences) for a medical term.\n","    - Strong system prompt forces brevity & English.\n","    - Slice by token length to avoid prompt-bleed.\n","    \"\"\"\n","    messages = [\n","        {\"role\": \"system\", \"content\":\n","         \"You are a clinical assistant. Provide a surgical procedures, accurate definition in English, \"\n","         \"limited to 1–2 sentences.\"},\n","        {\"role\": \"user\", \"content\": term}\n","    ]\n","\n","    # Build prompt with the model's chat template\n","    prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","    inputs = tok([prompt], return_tensors=\"pt\").to(model.device)\n","    with torch.no_grad():\n","        out = model.generate(\n","            **inputs,\n","            max_new_tokens=max_new_tokens,\n","            do_sample=not deterministic,   # for strict determinism set to False\n","            temperature=0.2,\n","            top_p=0.9,\n","            eos_token_id=tok.eos_token_id,\n","            pad_token_id=tok.pad_token_id\n","        )\n","\n","    # NEW: slice by token length (not by string length)\n","    gen_ids = out[0]\n","    new_ids = gen_ids[inputs[\"input_ids\"].shape[1]:]\n","    answer = tok.decode(new_ids, skip_special_tokens=True).strip()\n","\n","    # optional: ultra-concise post-trim (keep 2 sentences max)\n","    # import re\n","    # sents = re.split(r'(?<=[.!?])\\s+', answer)\n","    # answer = ' '.join(sents[:2]).strip()\n","\n","    return answer\n","\n","print(define_term_en(\"Vaginopexy by colposuspension\"))\n","\n"],"metadata":{"id":"dNVF7uXXcdCe","colab":{"base_uri":"https://localhost:8080/","height":159,"referenced_widgets":["bb3243ccb03d4a72898be90b9fade88e","16541ea085ff45d3848d74857d7ee45e","d7c78d5425c84962a89afbcce6bc36da","796694ce76a14d248283047d39d105e1","43e9c2fd62b54ab798645a9f60c0fabb","4df4227dfe284ed59351b80427bc70cd","b336a83b5b7143ebad9bd1643070601b","7ec9fe7fda884e1088a6f113d0938e14","dde33462c3e64cde967038fdfd4a5912","e4e17dc0cd154a2bb80d00d3bc64f377","0703501c769e4650ae61162866fc36a5"]},"executionInfo":{"status":"ok","timestamp":1760397429362,"user_tz":-540,"elapsed":36295,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"80b3aa60-039c-45c1-9509-d9acb6375be1"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb3243ccb03d4a72898be90b9fade88e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["<think>\n","\n","</think>\n","\n","Vaginopexy by colposuspension is a surgical procedure that involves suspending the vaginal vault to the sacroiliac joint to treat pelvic organ prolapse. It is performed through an abdominal incision and is used to restore anatomical support and function in the pelvic region.\n"]}]},{"cell_type":"markdown","source":["## 한국어 출력 버전"],"metadata":{"id":"UvQkYfeP9sjr"}},{"cell_type":"code","source":["# Inference: Ask the fine-tuned model to define a term\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from peft import PeftModel\n","import torch\n","\n","# ==== 경로 설정 ====\n","BASE_MODEL   = \"/content/drive/MyDrive/DILAB/qwen3-8b\"  # 베이스 모델(또는 허깅페이스 경로)\n","ADAPTER_DIR  = \"/content/drive/MyDrive/DILAB/OK/DI_LAB/MARS_Datathon/Models/qwen3_8b_snomed_lora\"  # LoRA 어댑터\n","USE_4BIT     = True\n","USE_BF16     = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n","\n","# ==== 모델/토크나이저 로드 ====\n","bnb = None\n","if USE_4BIT:\n","    bnb = BitsAndBytesConfig(\n","        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_compute_dtype=torch.bfloat16 if USE_BF16 else torch.float16\n","    )\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    BASE_MODEL,\n","    quantization_config=bnb,\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n","    dtype=torch.bfloat16 if USE_BF16 else torch.float16,  # torch_dtype deprec → dtype\n",")\n","tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, trust_remote_code=True)\n","if tok.pad_token is None:\n","    tok.pad_token = tok.eos_token\n","\n","# LoRA 어댑터 부착 (병합 모델이면 이 부분 건너뜀)\n","model = PeftModel.from_pretrained(model, ADAPTER_DIR)\n","model.eval()\n","\n","def define_term_ko(term: str, max_new_tokens: int = 200, deterministic: bool = True) -> str:\n","    # 1) 한국어 지시를 system에 명시\n","    messages = [\n","        {\"role\": \"system\", \"content\":\n","         \"당신은 임상 지식을 가진 의료 보조자입니다. \"\n","         \"사용자가 제시한 의학 용어를 한국어로 간결하고 정확하게 정의하세요. \"\n","         \"불필요한 서론/메모/추측은 금지하고 1~3문장으로 답하세요.\"},\n","        {\"role\": \"user\", \"content\": term}\n","    ]\n","    prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","    # 2) 토큰화\n","    inputs = tok([prompt], return_tensors=\"pt\").to(model.device)\n","\n","    # 3) 생성\n","    with torch.no_grad():\n","        out = model.generate(\n","            **inputs,\n","            max_new_tokens=max_new_tokens,\n","            do_sample=not deterministic,   # 정의문은 보통 결정적 생성 권장\n","            temperature=0.2,\n","            top_p=0.9,\n","            eos_token_id=tok.eos_token_id,\n","            pad_token_id=tok.pad_token_id\n","        )\n","\n","    # 4) \"토큰 길이\" 기준으로 신규 토큰만 추출 → 디코드\n","    gen_ids = out[0]\n","    new_token_ids = gen_ids[inputs[\"input_ids\"].shape[1]:]  # ← 핵심!\n","    answer = tok.decode(new_token_ids, skip_special_tokens=True).strip()\n","    return answer\n","\n","# === 사용 예시 ===\n","print(define_term_ko(\"Asthma\"))\n","print(define_term_ko(\"Myocardial infarction\"))\n"],"metadata":{"id":"QfQdsy8A6ui9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hGITGSGd92CF"},"execution_count":null,"outputs":[]}]}